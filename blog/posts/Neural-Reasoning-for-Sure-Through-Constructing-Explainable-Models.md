**Paper Link:**[Neural Reasoning For Sure Through Constructing Explainable Models]("https://ojs.aaai.org/index.php/AAAI/article/view/33262")

**Paper Authors:** Tiansi Dong, Mateja Jamnik and Pietro Liò.

The current flaw with neural network reasoning is due to it being seen as a “black box” system where the computation of how the output is generated from the input is hidden. This consequently often classifies reasoning from the outputs as “unsure”, which in undesirable in many situations that require complete certainty when models respond to real life applications.

This paper explores ways to make neural network reasoning more reliable by introducing the reasoning-for-sure system. This system focuses on Aristotelian syllogism: a deductive reasoning structure in which a conclusion is derived from two or more propositions that are asserted or assumed to be true, ensuring the reliability of the “for-sure” property persists.
